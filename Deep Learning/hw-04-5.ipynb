{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from IPython.display import display, HTML\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset from the CSV file\n",
    "data = pd.read_csv('malware_BinaryImbalanced.csv')\n",
    "\n",
    "# Selecting relevant columns based on ReadMe.txt\n",
    "cols = ['classification', 'os', 'usage_counter', 'prio', 'static_prio', 'normal_prio', 'vm_pgoff', \n",
    "        'vm_truncate_count', 'task_size', 'map_count', 'hiwater_rss', 'total_vm', 'shared_vm',\n",
    "        'exec_vm', 'reserved_vm', 'nr_ptes', 'nvcsw', 'nivcsw', 'signal_nvcsw']\n",
    "df = data[cols]\n",
    "\n",
    "# strip column names\n",
    "df = df.rename(columns=lambda x: x.strip())\n",
    "cols = df.columns\n",
    "\n",
    "# replace missing values in numerical variables by using mean value\n",
    "for col in df.columns:\n",
    "    if pd.api.types.is_numeric_dtype(df[col]):\n",
    "        df[col].fillna(df[col].mean(), inplace=True)\n",
    "\n",
    "# encode labels\n",
    "y = df['classification']\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y)\n",
    "y_encoded = le.transform(y)\n",
    "df['classification'] = y_encoded\n",
    "\n",
    "# convert all nominal variables to binary variables\n",
    "df_num = df.copy(deep=True)\n",
    "df_dummies = pd.get_dummies(df_num[['os']])\n",
    "df_num = df_num.join(df_dummies)\n",
    "df_num = df_num.drop('os', axis=1)\n",
    "df_num = df_num.drop('os_Windows', axis=1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X = df_num.drop('classification', axis=1)\n",
    "y = df_num['classification']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize models\n",
    "mlp = MLPClassifier(max_iter=300)\n",
    "rf = RandomForestClassifier()\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "# Define hyperparameters for each model\n",
    "mlp_params = {'hidden_layer_sizes': [(50,), (100,), (50,50)],\n",
    "                    'activation': ['relu', 'tanh'],\n",
    "                    'solver': ['adam', 'sgd']}\n",
    "\n",
    "rf_params = {'n_estimators': [100, 200], 'max_depth': [10, 20, 30]}\n",
    "\n",
    "xgb_params = {'n_estimators': [100, 200], 'max_depth': [3, 6, 10]}\n",
    "\n",
    "# Grid Search for MLP\n",
    "mlp_grid = GridSearchCV(mlp, mlp_params, scoring='f1', cv=5)\n",
    "mlp_grid.fit(X_train_scaled, y_train)\n",
    "mlp_best = mlp_grid.best_estimator_\n",
    "\n",
    "# Grid Search for Random Forest\n",
    "rf_grid = GridSearchCV(rf, rf_params, scoring='f1', cv=5)\n",
    "rf_grid.fit(X_train_scaled, y_train)\n",
    "rf_best = rf_grid.best_estimator_\n",
    "\n",
    "# Grid Search for XGBoost\n",
    "xgb_grid = GridSearchCV(xgb, xgb_params, scoring='f1', cv=5)\n",
    "xgb_grid.fit(X_train_scaled, y_train)\n",
    "xgb_best = xgb_grid.best_estimator_\n",
    "\n",
    "# Predictions and Metrics\n",
    "models = {'MLP': mlp_best, 'Random Forest': rf_best, 'XGBoost': xgb_best}\n",
    "results = {}\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "    results[name] = {'Accuracy': accuracy, 'F1 Score': f1, 'AUC': auc_score, 'Best Parameters': model.get_params()}\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Comparison')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Display Results\n",
    "print(\"Model Comparison Results:\")\n",
    "for name, metrics in results.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        if metric != 'Best Parameters':\n",
    "            print(f\"  {metric}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"  Best Parameters: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
