{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings (ignore any warnings during the execution)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for data processing and modeling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from IPython.display import display, HTML\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 36 columns):\n",
      " #   Column             Non-Null Count   Dtype \n",
      "---  ------             --------------   ----- \n",
      " 0   hash               100000 non-null  object\n",
      " 1   millisecond        100000 non-null  int64 \n",
      " 2   classification     100000 non-null  object\n",
      " 3   os                 100000 non-null  object\n",
      " 4   state              100000 non-null  int64 \n",
      " 5   usage_counter      100000 non-null  int64 \n",
      " 6   prio               100000 non-null  int64 \n",
      " 7   static_prio        100000 non-null  int64 \n",
      " 8   normal_prio        100000 non-null  int64 \n",
      " 9   policy             100000 non-null  int64 \n",
      " 10  vm_pgoff           100000 non-null  int64 \n",
      " 11  vm_truncate_count  100000 non-null  int64 \n",
      " 12  task_size          100000 non-null  int64 \n",
      " 13  cached_hole_size   100000 non-null  int64 \n",
      " 14  free_area_cache    100000 non-null  int64 \n",
      " 15  mm_users           100000 non-null  int64 \n",
      " 16  map_count          100000 non-null  int64 \n",
      " 17  hiwater_rss        100000 non-null  int64 \n",
      " 18  total_vm           100000 non-null  int64 \n",
      " 19  shared_vm          100000 non-null  int64 \n",
      " 20  exec_vm            100000 non-null  int64 \n",
      " 21  reserved_vm        100000 non-null  int64 \n",
      " 22  nr_ptes            100000 non-null  int64 \n",
      " 23  end_data           100000 non-null  int64 \n",
      " 24  last_interval      100000 non-null  int64 \n",
      " 25  nvcsw              100000 non-null  int64 \n",
      " 26  nivcsw             100000 non-null  int64 \n",
      " 27  min_flt            100000 non-null  int64 \n",
      " 28  maj_flt            100000 non-null  int64 \n",
      " 29  fs_excl_counter    100000 non-null  int64 \n",
      " 30  lock               100000 non-null  int64 \n",
      " 31  utime              100000 non-null  int64 \n",
      " 32  stime              100000 non-null  int64 \n",
      " 33  gtime              100000 non-null  int64 \n",
      " 34  cgtime             100000 non-null  int64 \n",
      " 35  signal_nvcsw       100000 non-null  int64 \n",
      "dtypes: int64(33), object(3)\n",
      "memory usage: 27.5+ MB\n",
      "None\n",
      "                                          hash  millisecond classification  \\\n",
      "0  com.kmcpesh.medicalskillsproceduresfree.apk            0         benign   \n",
      "1  com.kmcpesh.medicalskillsproceduresfree.apk            1         benign   \n",
      "2  com.kmcpesh.medicalskillsproceduresfree.apk            2         benign   \n",
      "3  com.kmcpesh.medicalskillsproceduresfree.apk            3         benign   \n",
      "4  com.kmcpesh.medicalskillsproceduresfree.apk            4         benign   \n",
      "\n",
      "       os  state  usage_counter        prio  static_prio  normal_prio  policy  \\\n",
      "0  Ubuntu      0              0  3069403136        16447            0       0   \n",
      "1  CentOS      0              0  3069403136        16447            0       0   \n",
      "2  Ubuntu      0              0  3069403136        16447            0       0   \n",
      "3  CentOS      0              0  3069403136        16447            0       0   \n",
      "4     Mac      0              0  3069403136        16447            0       0   \n",
      "\n",
      "   ...  nivcsw  min_flt  maj_flt  fs_excl_counter        lock   utime  stime  \\\n",
      "0  ...       0        1      120                0  3204448256  384502      5   \n",
      "1  ...       0        1      120                0  3204448256  384502      5   \n",
      "2  ...       0        1      120                0  3204448256  384502      5   \n",
      "3  ...       0        1      120                0  3204448256  384502      5   \n",
      "4  ...       0        1      120                0  3204448256  384502      5   \n",
      "\n",
      "   gtime  cgtime  signal_nvcsw  \n",
      "0      0       0             0  \n",
      "1      0       0             0  \n",
      "2      0       0             0  \n",
      "3      0       0             0  \n",
      "4      0       0             0  \n",
      "\n",
      "[5 rows x 36 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "# Load the dataset 'malware_BinaryImbalanced.csv' into a pandas DataFrame\n",
    "data = pd.read_csv('malware_BinaryImbalanced.csv')\n",
    "\n",
    "# Display dataset info\n",
    "print(data.info())\n",
    "print(data.head())\n",
    "\n",
    "# Selecting relevant columns as per the ReadMe.txt (list of columns that are relevant)\n",
    "cols = ['classification', 'os', 'usage_counter', 'prio', 'static_prio', 'normal_prio', 'vm_pgoff', \n",
    "        'vm_truncate_count', 'task_size', 'map_count', 'hiwater_rss', 'total_vm', 'shared_vm',\n",
    "        'exec_vm', 'reserved_vm', 'nr_ptes', 'nvcsw', 'nivcsw', 'signal_nvcsw']\n",
    "df = data[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean column names\n",
    "# Strip any extra spaces in column names\n",
    "df = df.rename(columns=lambda x: x.strip())\n",
    "cols = df.columns  # Update column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Handle missing values\n",
    "# Replace missing (NaN) values in numerical columns with the column's mean value\n",
    "for col in df.columns:\n",
    "    if pd.api.types.is_numeric_dtype(df[col]):\n",
    "        df[col].fillna(df[col].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the target variable 'classification'\n",
    "# Label encode the 'classification' column from categorical values to numeric\n",
    "y = df['classification']\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y)  # Fit the label encoder\n",
    "y_encoded = le.transform(y)  # Encode labels (0 or 1)\n",
    "df['classification'] = y_encoded  # Replace original classification column with encoded values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical variable ('os') into binary dummy variables\n",
    "# 'os' is a categorical column, which we need to convert to binary dummy variables\n",
    "df_num = df.copy(deep=True)  # Make a deep copy to avoid modifying the original df\n",
    "df_dummies = pd.get_dummies(df_num[['os']])  # Create dummy variables for 'os'\n",
    "df_num = df_num.join(df_dummies)  # Add the dummy variables back to the dataframe\n",
    "df_num = df_num.drop('os', axis=1)  # Drop the original 'os' column\n",
    "df_num = df_num.drop('os_Windows', axis=1)  # Drop one of the dummy variables ('os_Windows') to avoid multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features (X) and target (y)\n",
    "# X contains the features (independent variables), y is the target (dependent variable)\n",
    "X = df_num.drop('classification', axis=1)  # Features are all columns except 'classification'\n",
    "y = df_num['classification']  # Target is 'classification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the dataset into a training set (75%) and a testing set (25%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Scale the data\n",
    "# Standardize features by scaling them to have a mean of 0 and standard deviation of 1\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Fit the scaler to training data and transform it\n",
    "X_test_scaled = scaler.transform(X_test)  # Use the fitted scaler to transform the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "# Initialize the machine learning models we will use\n",
    "mlp = MLPClassifier(max_iter=300)  # Multi-layer Perceptron (neural network)\n",
    "rf = RandomForestClassifier()  # Random Forest classifier\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')  # XGBoost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters for each model\n",
    "# These are the hyperparameters we will tune using GridSearchCV\n",
    "mlp_params = {'hidden_layer_sizes': [(50,), (100,), (50,50)],\n",
    "                    'activation': ['relu', 'tanh'],\n",
    "                    'solver': ['adam', 'sgd']}\n",
    "\n",
    "rf_params = {'n_estimators': [100, 200], 'max_depth': [10, 20, 30]}\n",
    "\n",
    "xgb_params = {'n_estimators': [100, 200], 'max_depth': [3, 6, 10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning using Grid Search\n",
    "# GridSearchCV performs cross-validation to find the best hyperparameters\n",
    "\n",
    "# Grid Search for MLP (Neural Network)\n",
    "mlp_grid = GridSearchCV(mlp, mlp_params, scoring='f1', cv=5)  # Use F1 score as the evaluation metric\n",
    "mlp_grid.fit(X_train_scaled, y_train)  # Train the model with the training data\n",
    "mlp_best = mlp_grid.best_estimator_  # Get the best model after tuning\n",
    "\n",
    "# Grid Search for Random Forest\n",
    "rf_grid = GridSearchCV(rf, rf_params, scoring='f1', cv=5)\n",
    "rf_grid.fit(X_train_scaled, y_train)\n",
    "rf_best = rf_grid.best_estimator_\n",
    "\n",
    "# Grid Search for XGBoost\n",
    "xgb_grid = GridSearchCV(xgb, xgb_params, scoring='f1', cv=5)\n",
    "xgb_grid.fit(X_train_scaled, y_train)\n",
    "xgb_best = xgb_grid.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models and compare performance\n",
    "# Store the results of each model and evaluate them\n",
    "models = {'MLP': mlp_best, 'Random Forest': rf_best, 'XGBoost': xgb_best}\n",
    "results = {}\n",
    "\n",
    "# Initialize the plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Loop through each model, make predictions, and calculate performance metrics\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test_scaled)  # Predict the target labels for the test set\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]  # Get probabilities for the positive class\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)  # Calculate accuracy\n",
    "    f1 = f1_score(y_test, y_pred)  # Calculate F1 score\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)  # Calculate ROC AUC score\n",
    "\n",
    "    results[name] = {'Accuracy': accuracy, 'F1 Score': f1, 'AUC': auc_score, 'Best Parameters': model.get_params()}\n",
    "\n",
    "    # Calculate ROC curve and plot it\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)  # False positive rate, true positive rate for ROC curve\n",
    "    roc_auc = auc(fpr, tpr)  # Calculate the AUC from the ROC curve\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')  # Plot the ROC curve for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC plot\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line for random classifier\n",
    "plt.xlim([0.0, 1.0])  # Set axis limits\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')  # Label for the x-axis\n",
    "plt.ylabel('True Positive Rate')  # Label for the y-axis\n",
    "plt.title('ROC Curve Comparison')  # Plot title\n",
    "plt.legend(loc=\"lower right\")  # Display the legend\n",
    "plt.show()  # Show the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Display Results\n",
    "# Print out the performance metrics of each model\n",
    "print(\"Model Comparison Results:\")\n",
    "for name, metrics in results.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        if metric != 'Best Parameters':\n",
    "            print(f\"  {metric}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"  Best Parameters: {value}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
